{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 引入依赖库"
      ],
      "metadata": {
        "id": "56ZbKKbC-9kj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vizQha1o-i8z"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install transformers sentencepiece\n",
        "!pip install d2l\n",
        "!pip instal matplotlib==3.0.0\n",
        "!pip install matplotlib_inline\n",
        "!pip install rouge\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch..utils.data import Dataset, DataLoader\n",
        "from d2l import torch as d2l\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "from rouge import Rouge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart = AutoModelForSeq2SeqLM.from_pretrained('sshleifer/distilbart-xsum-12-6')\n",
        "tokenizer = AutoTokenizer.from_pretrained('sshleifer/distilbart-xsum-12-6')"
      ],
      "metadata": {
        "id": "RZIoOEz5Ao4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 超参数设定\n",
        "\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "devices = d2l.try_all_gpus()\n",
        "hidden_size = 512\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.15\n",
        "num_epochs = 4\n"
      ],
      "metadata": {
        "id": "D_ZbV5j0TtPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据预处理 数据迭代器装载"
      ],
      "metadata": {
        "id": "gWTIByzO--TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path='/content/news_summary/train_dataset.csv'\n",
        "eval_path='/content/news_summary/eval_dataset.csv'\n",
        "test_path='/content/news_summary/test_dataset.csv'\n",
        "\n",
        "def read_data(root, is_test):\n",
        "    if is_test:\n",
        "        df = pd.DataFrame([],columns=[\"Index\",\"Text\"])\n",
        "    else:\n",
        "        df = pd.DataFrame([],columns=[\"Index\",\"Text\",\"Summary\"])\n",
        "\n",
        "    f = open(root,'r',encoding='utf-8-sig').readlines()\n",
        "    for idx, texts in enumerate(f):\n",
        "        df.loc[idx] = text..split(\"\\t\")\n",
        "\n",
        "    return df\n",
        "\n",
        "train = read_data(train_root, False)\n",
        "eval = read_data(eval_root, False)\n",
        "test = read_data(test_root, True)"
      ],
      "metadata": {
        "id": "GJ8zn9JG-94Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
        "\n",
        "\n",
        "class News_summary(Dataset):\n",
        "    def __init__(self, dataset, mode):\n",
        "        self.mode = mode\n",
        "        \n",
        "        if mode == 'test':\n",
        "            self.text = dataset[\"Text\"]\n",
        "        else:\n",
        "            self.text, self.summary = dataset[\"Text\"], dataset[\"Summary\"]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if mode == 'test':\n",
        "            self.text[idx] = self.text[idx].split('updated :')[-1].strip()\n",
        "            self.text[idx] = self.text[idx].split('-lrb- cnn -rrb- --')[-1].strip()\n",
        "\n",
        "            self.text[idx] = tokenizer(WHITESPACE_HANDLER,\n",
        "                           self.text[idx], max_length=1768,\n",
        "                           return_tensors=\"pt\",\n",
        "                           padding=True, truncation=True\n",
        "                           )[\"input_ids\"]\n",
        "            return self.text[idx]\n",
        "        else:\n",
        "            self.text[idx] = self.text[idx].split('updated :')[-1].strip()\n",
        "            self.text[idx] = self.text[idx].split('-lrb- cnn -rrb- --')[-1].strip()\n",
        "\n",
        "            self.text[idx] = tokenizer(WHITESPACE_HANDLER,\n",
        "                           self.text[idx], max_length=1768,\n",
        "                           return_tensors=\"pt\",\n",
        "                           padding=True, truncation=True)\n",
        "            self.summary[idx] = tokenizer(self.abst[idx], max_length=512,\n",
        "                            padding=True, truncation=True\n",
        "                            )[\"input_ids\"]\n",
        "            return self.text[idx], self.summary[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n"
      ],
      "metadata": {
        "id": "8Xj-W568_Ww5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = News_summary(train, mode='train')\n",
        "eval_set = News_summary(eval, mode='eval')\n",
        "test_set = News_summary(test, mode='test')\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size,\n",
        "              shuffle=True, num_workers=4)\n",
        "eval_loader = DataLoader(eval_set, batch_size=batch_size,\n",
        "              shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "O5OtxLHkSxMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型"
      ],
      "metadata": {
        "id": "RPA3-Y_nUIkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Distilbart_gru(nn.Module):\n",
        "    def __init__(self, bart, hidden_size, vocab_size, n_layers,\n",
        "                 bidirectional, dropout, **kwargs):\n",
        "        super(TextCNN, self).__init__(**kwargs)\n",
        "\n",
        "        self.bart = bart\n",
        "        embedding_size = bart.config.to_dict()['hidden_size']\n",
        "\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers=n_layers,\n",
        "                  bidirectional=bidirectional, dropout=dropout)\n",
        "        self.Linear = nn.Linear(hidden_size*2 if bidirectional else hidden_size\n",
        "                     , vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            embed = self.bart(x)[0]\n",
        "\n",
        "        _, hidden = self.gru(embed)\n",
        "\n",
        "        if self.gru.bidirectional:\n",
        "            hidden = self.dropout(torch.cat(hidden[-2,:,:],\n",
        "                             hidden[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "        \n",
        "        output = self.Linear(hidden)\n",
        "        return output"
      ],
      "metadata": {
        "id": "I2cNXJIjUI_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "31EVzNxpaitC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = d2l.Vocab()\n",
        "vocab.idx_to_token = json.load(open('/content/news_summary/distilbart-xsum-12-6/vocab.json'))\n",
        "vocab.token_to_idx = {token: idx for idx, token in\n",
        "                    enumerate(vocab.idx_to_token)}"
      ],
      "metadata": {
        "id": "krb2Q47-aCRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 定义LOSS和Rouge"
      ],
      "metadata": {
        "id": "1rOf_BZWbEmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Distilbart_gru(bart, hidden_size, len(vocab), bidirectional, dropout)\n",
        "\n",
        "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "trainer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def print_rouge_L(output, label):\n",
        "    rouge = Rouge()\n",
        "    rouge_score = rouge.get_scores(output, label)\n",
        "\n",
        "    rouge_L_f1 = 0\n",
        "    rouge_L_p = 0\n",
        "    rouge_L_r = 0\n",
        "    for d in rouge_score:\n",
        "        rouge_L_f1 += d[\"rouge-l\"][\"f\"]\n",
        "        rouge_L_p += d[\"rouge-l\"][\"p\"]\n",
        "        rouge_L_r += d[\"rouge-l\"][\"r\"]\n",
        "    print(\"rouge_f1:%.2f\" % (rouge_L_f1 / len(rouge_score)))\n",
        "    print(\"rouge_p:%.2f\" % (rouge_L_p / len(rouge_score)))\n",
        "    print(\"rouge_r:%.2f\" % (rouge_L_r / len(rouge_score)))\n"
      ],
      "metadata": {
        "id": "rL99NfTDbE6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train ＆ eval"
      ],
      "metadata": {
        "id": "E5Yj48iOetp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_(net, data_iter, lr, num_epochs, device):\n",
        "\n",
        "    def xavier_init_weights(m):\n",
        "        if type(m) == nn.Linear:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "        if type(m) == nn.GRU:\n",
        "            for param in m._flat_weights_names:\n",
        "                if \"weight\" in param:\n",
        "                    nn.init.xavier_uniform_(m._parameters[param])\n",
        "    net.apply(xavier_init_weights)\n",
        "    net.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    net.train()\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                 xlim=[10, num_epochs])\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        timer = d2l.Timer()\n",
        "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "        for batch in data_iter:\n",
        "            optimizer.zero_grad()\n",
        "            X, Y = [x.to(device) for x in batch]\n",
        "            Y_hat = net(X)\n",
        "            l = loss(Y_hat, Y)\n",
        "            l.sum().backward()  # Make the loss scalar for `backward`\n",
        "            d2l.grad_clipping(net, 1)\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                metric.add(l.sum())\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
        "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
        "          f'tokens/sec on {str(device)}')\n",
        "    \n",
        "\n",
        "def eval_(net, eval_iter):\n",
        "    net.eval()\n",
        "    net.parameters().requires_grad = False\n",
        "    summary = []\n",
        "    for batch in data_iter:\n",
        "        X, Y = [x.to(device) for x in batch]\n",
        "        Y_hat = net(X)\n",
        "        dec_X = Y_hat.argmax(dim=2)\n",
        "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
        "        output_seq = []\n",
        "        output_seq.append(pred)\n",
        "        summary.append(vocab.to_tokens(output_seq))\n",
        "\n",
        "    print_rouge_L(summary, eval[\"Summary\"])"
      ],
      "metadata": {
        "id": "JYbNhQJ-et8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_(model, train_loader, lr, num_epochs, devices)"
      ],
      "metadata": {
        "id": "g2y9jYbUok23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_(model, eval_loader)\n",
        "\n",
        "# 保存模型\n",
        "torch.save(model.state_dict(), '/content/news_summary/model_weights.txt')"
      ],
      "metadata": {
        "id": "fIoVjsjToqG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预测"
      ],
      "metadata": {
        "id": "QwuZAwAZo7mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.load('/content/THUCNews/model_weights.txt')\n",
        "model.load_state_dict(params)\n",
        "\n",
        "fw = open('/content/news_summary/submission.csv', 'w+', encoding='utf-8-sig')\n",
        "\n",
        "for idx,text.to(devices) in tqdm(enumerate(test_set), total=1000):\n",
        "    model.eval()\n",
        "    net.parameters().requires_grad = False\n",
        "    Y_hat = model(text)\n",
        "    dec_X = Y_hat.argmax(dim=2)\n",
        "    pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
        "    output_seq = []\n",
        "    output_seq.append(pred)\n",
        "    summary = vocab.to_tokens(output_seq)\n",
        "\n",
        "    fw = open('/content/news_summary/submission.csv', 'a+')\n",
        "    fw.write(str(idx))\n",
        "    fw.write('\\t')\n",
        "    fw.write(summary)\n",
        "    fw.write('\\n')\n",
        "\n",
        "fw.close()"
      ],
      "metadata": {
        "id": "nriEKttpo72o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}